\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{amssymb}
\usepackage{framed}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}


\setlength{\parindent}{0pt}
\setlength{\parskip}{3ex}

\begin{document}

\begin{center}
  {\large Artificial Neural Networks and Deep Architectures, DD2437}\\
  \vspace{7mm}
  {\huge Short report on lab assignment 1\\[1ex]}
  {\Large Learning and generalisation in feed-forward networks ---\\[1ex]
 from perceptron learning to backprop}\\
  \vspace{8mm}  
  {\Large Ivan Stresec, Frano Rajic\\}
  \vspace{4mm}
  {\large September 11, 2020\\}
\end{center}

%\begin{framed}
%Please be aware of the constraints for this document. The main intention here is that you learn how to select and organise the most relevant information into a concise and coherent report. The upper limit for the number of pages is 6 with fonts and margins comparable to those in this template and no appendices are allowed. \\
%These short reports should be submitted to Canvas by the authors as a team before the lab presentation is made. To claim bonus points the authors should uploaded their short report a day before the bonus point deadline. The report can serve as a support for your lab presentation, though you may put emphasis on different aspects in your oral demonstration in the lab.
%Below you find some extra instructions in italics. Please remove them and use normal font for your text.
%\end{framed}

\section{Main objectives and scope of the assignment}

%\textit{List here a concise list of your major intended goals, what you planned to do and what you wanted to learn/what problems you were set to address or investigate, e.g.}\\
Our major goals in the assignment were  
\begin{itemize}
\item to investigate the performance of classification single-layer perceptrons (SLPs) in various settings
\item to observe the capabilities of MLPs in the context of solving non-linearly separable patterns in both classification and regression problems 
\item to implement MLPs in a chaotic time-series prediction\footnote{FIX THIS ONE}
\end{itemize}

%\textit{Then you can write two or three sentences about the scope, limitations and assumptions made for the lab assignment}\\

\section{Methods}

%\textit{Mention here in just a couple of sentences what tools you have used, e.g. programming/scripting environment, toolboxes. If you use some unconventional method or introduce a clearly different performance measure, you can briefly mention or define it here.}\\

For the first assignment we used Python as a programming language and its library Numpy for data representation and matrix operations. For graphs we used the Matplotlib library. In the second assignment we used Pytorch, as well as the aforementioned libraries. As an IDE, we've used JetBrains' PyCharm for both assignments.

\section{Results and discussion - Part I}

%\begin{framed}
%\textit{Make effort to be \textbf{concise and to the point} in your story of what you have done, what you have observed and demonstrated, and in your responses to specific questions in the assignment. You should skip less important details and explanations. In addition, you are requested to add a \textbf{discussion} about your interpretations/predictions or other thoughts concerned with specific tasks in the assignment. This can boil down to just a few bullet points or a couple of sentences for each section of your results. \\ Overall, structure each Results section as you like, e.g. in points. Analogously, feel free to group and combine answers to the questions, even between different experiments, e.g. with linearly separable and non-separable data, if it makes your story easier to convey. \\
%\\Plan your sections and consider making combined figures with subplots rather than a set of separate figures. \textbf{Figures} have to condense information, e.g. there is no point showing a separate plot for generated data and then for a decision boundary, this information can be contained in a single plot. Always carefully describe the axes, legends and add meaningful captions. Keep in mind that figures serve as a support for your description of the key findings (it is like storytelling but in technical format and academic style. \\
%\\Similarly, use \textbf{tables} to group relevant results for easier communication but focus on key aspects, do not overdo it. All figures and tables attached in your report must be accompanied by captions and referred to in the text, e.g. $"$in Fig.X or Table Y one can see ....$"$. \\
%\\When you report quantities such as errors or other performanc measures, round numbers to a reasonable number of decimal digits (usually 2 or 3 max). Apart from the estimated mean values, obtained as a result of averaging over multiple simulations, always include also \textbf{the second moment}, e.g. standard deviation (S.D.). The same applies to some selected plots where \textbf{error bars} would provide valuable information, especially where conclusive comparisons are drawn.} 
%\end{framed}

\subsection{Classification with a single-layer perceptron \textit{(ca.1 page)}}
\textit{Combine results and findings from perceptron simulations on both linearly separable and non-separable datasets. Answer the questions, quantify the outcomes, discuss your interpretations and summarise key findings as conclusions.}

In solving classification problem using an SLP one can easily observe its efficiency and its limitations.

Firstly, we have implemented the perceptron rule (implying a sequential learning scheme) and the Delta learning rule (batch learning) to a randomly generated, linearly separable classification dataset. Both have performed well and converged quickly...

Secondly, we have tested the differences between batch and sequential learning for the Delta rule SLP. In terms of epochs, both approaches converge relatively quickly, with ... being somewhat quicker

Thirdly, we have tested removing the bias term with the Delta rule in batch mode. Even without testing it, it is clear that the algorithm can converge only when the data is linearly separable by a line which goes through the center of the coordinate system. 



\subsection{Classification and regression with a two-layer perceptron \textit{(ca.2 pages)}}

\subsubsection{Classification of linearly non-separable data}
%\textit{It seems that one (decision boundary) or two plots (inclusing learning curves) should suffice. Build a story around the questions in the assignment. Include concise motivation for your findings and potential interpretations/speculations.}

\subsubsection{The encoder problem}
%\textit{Here you do not really need any illustrations, this could be a very short section reporting on your experiments in line with the assignment questions.}

\subsubsection{Function approximation}
%\textit{This subsection requires plots to reflect intuitive visual interpretation of the results. Make sure that you condense information and avoid any excessive plotting. Here you might also need to incorporate some illustration of the network's generalisation performance or use a table to systematically report the results requested in the assignment.}

\section{Results and discussion - Part II \textit{(ca.2 pages)}}

%\textit{Here you do not have to introduce the problem or define Mackey-Glass time series, as you should focus on the results. You could divide them into two parts as the following two suggested subsections but you might as well keep your story under the main heading of Part II of the assignment. Importantly, always clearly state what network architecture you use, crucially with the number of hidden nodes, systematically report average results with various manipulations (regularisation etc.) and pay attention to differences between training, validation and test errors. Illustrating the outcome of your network predictions along with the original chaotic time series can also be very helpful. Finally, since you compare two- and three-layer architectures, make sure that you do not jump to any conclusions based on a small number of simulations unless you have statistically convincing evidence (when you comare the mean performance measures, their second moment is also relevant). In this part it may be particularly desirable to rely on tables.}

For this assignment, we have used the Mackey-Glass time series to evaluate a two-layer perceptron network. The data was simply generated by using the starting conditions and sequentially calculating values using the iterative formula given in the assignment. Furthermore, the data was split into three consecutive non-overlapping blocks for training, validation, and testing using 800, 200, and 200 values, respectively. For the regularisation method we have used the standard L2 norm. Early stopping is implemented in such a way that if improvement (by at least $10^{-5}$) in the validation mean square error (MSE) is not visible for 50 iterations the learning stops. Also, the maximum number of epochs allowed is capped at 5000.

\subsection{Two-layer perceptron for time series prediction - model selection, regularisation and validation}
To test the influence of regularization methods on weight distribution we have used regularization rates of 0, 0.01, 0.1 and 1 with 10 networks with 8 nodes in the hidden layer each, and made histograms using the accumulated weights for each regularization rate. The histograms can be seen in figure \ref{fig:weight_histogram}.

\begin{figure*}[h!]
	\centering
	\begin{subfigure}[b]{0.475\textwidth}
		\centering
		\includegraphics[width=\textwidth]{images/weight_histogram_0.png}
		\caption[Network2]%
		{{\small No regularization}}    
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.475\textwidth}  
		\centering 
		\includegraphics[width=\textwidth]{images/weight_histogram_0_01.png}
		\caption[]%
		{{\small L2 regularization with $\lambda = 0.01$}}   
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.475\textwidth}   
		\centering 
		\includegraphics[width=\textwidth]{images/weight_histogram_0_1.png}
		\caption[]%
		{{\small L2 regularization with $\lambda = 0.1$}}    
	\end{subfigure}
	\begin{subfigure}[b]{0.475\textwidth}   
		\centering 
		\includegraphics[width=\textwidth]{images/weight_histogram_1.png}
		\caption[]%
		{{\small L2 regularization with $\lambda = 1$}}    
	\end{subfigure}
	\caption{The distributions of weights for different regularization strengths} 
	\label{fig:weight_histograms}
\end{figure*}

The differences between the distributions are not too clear, and this can probably be attributed to the fact that a simple two-layer network such as ours does not suffer from overfitting, and as such has no problems with overly large weights. Generally, networks which use higher regularization strength should stick closer to smaller weights, while those which do not show a more even distribution. This can be attributed to the fact that a penalty-based regularization, such as L2, forces weights to be smaller by 'punishing' their size in the cost function (which can be seen in equation \ref{eqn:l2_regularization}) and in turn reduce the complexity of the model.

\begin{equation}
\label{eqn:l2_regularization}
		cost = MSE + \lambda \sum_{i=1}^{n} \sum_{j=1}^{m} w_{i,j}^2
\end{equation}

Table \ref{table:learning_outcomes} shows the mean and standard deviation values of the loss function (MSE) for several configurations. Generally, networks with a lower number of nodes in the hidden layer tend to have larger errors, probably due to an overly simple model, i.e. underfitting. For experiments we have used a fixed L2 regularization with $\lambda = 0.1$.

\begin{table}
	\centering
	\begin{tabular}{|p{1.5cm}|p{2cm}||p{2cm}|p{1.5cm}|}
		\hline
		nodes&learning rate&MSE mean&MSE std\\
		\hline
		2 & 0.01 & 0.xxx & 0.xxx \\ \hline
		2 & 0.1 & 0.01488 & 0.00220 \\ \hline
		4 & 0.01 & 0.xxx & 0.xxx \\ \hline
		4 & 0.1 & 0.01556 & 0.00295 \\ \hline
		6 & 0.01 & 0.xxx & 0.xxx \\ \hline
		6 & 0.1 & 0.01488 & 0.00210 \\ \hline
		8 & 0.01 & 0.xxx & 0.xxx \\ \hline
		8 & 0.1 & 0.01354 & 0.00236 \\ \hline
	\end{tabular}
	\label{table:learning_outcomes}
	\caption{Evaluation of various network configurations (L2 - $\lambda = 0.1$)}	
\end{table}


Finally, we have decided to use a network with a learning rate of 0.1 with X nodes in the hidden layer, as well as the aforementioned L2 regularization with a rate of 0.1 to evaluate the final solution. Figure \ref{fig:learning_process} shows error rates of such a network on the training and validation set, and figure \ref{fig:network_outputs} shows the predictions of the network alongside the data itself.

% We have also noticed that networks sometimes get stuck in local minima where they average the target outputs from the training set, instead of modelling the relationship between input and target output. This problem could probably be solved by careful observation of weight initialization as well as using momentum, minibatch learning and well-known optimization methods such as RMSprop, Adam, etc.



The network shows good generalisation on the test set and seems to have no trouble with overfitting.



\subsection{Comparison of two- and three-layer perceptron for noisy time series prediction}

\section{Final remarks \normalsize{\textit{(max 0.5 page)}}}
% \textit{Please share your final reflections on the lab, its content and your own learning. Which parts of the lab assignment did you find confusing or not necessarily helping in understanding important concepts and which parts you have found interesting and relevant to your learning experience? \\
% Here you can also formulate your opinion, interpretation or speculation about some of the simulation outcomes. Please add any follow-up questions that you might have regarding the lab tasks and the results you have produced.}

\end{document}
